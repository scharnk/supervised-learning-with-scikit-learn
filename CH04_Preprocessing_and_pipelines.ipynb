{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH04_Preprocessing-and-pipelines.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scharnk/supervised-learning-with-scikit-learn/blob/master/CH04_Preprocessing_and_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPHOxJ0JH5TQ",
        "colab_type": "text"
      },
      "source": [
        "#Dealing with categorical features\n",
        "\n",
        "* need to use numerical dummy variables\n",
        ">* 0 = NOT\n",
        ">* 1 = IS\n",
        "\n",
        "* **Categorical feature values = data - 1** (remove implicit features so as not to duplicate) \n",
        "\n",
        "* Can use:\n",
        ">* sklearn --> OneHotEncoder()\n",
        ">* pandas --> get_dummies()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJjgs7gYIOOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read 'gapminder.csv' into a DataFrame: df\n",
        "df = pd.read_csv('gapminder.csv')\n",
        "\n",
        "# Create a boxplot of life expectancy per region\n",
        "df.boxplot(('life'), ('Region'), rot=60)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPO_uUkrL2rM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dummy variables: df_region\n",
        "df_region = pd.get_dummies(df)\n",
        "\n",
        "# Print the columns of df_region\n",
        "print(df_region.columns)\n",
        "\n",
        "# Create dummy variables with drop_first=True: df_region\n",
        "df_region = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Print the new columns of df_region\n",
        "print(df_region.columns)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZlD2LwOL2i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary modules\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Instantiate a ridge regressor: ridge\n",
        "ridge = Ridge(alpha=0.5, normalize=True)\n",
        "\n",
        "# Perform 5-fold cross-validation: ridge_cv\n",
        "ridge_cv = cross_val_score(ridge, X, y, cv=5)\n",
        "\n",
        "# Print the cross-validated scores\n",
        "print(ridge_cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6wa9VYlOWQi",
        "colab_type": "text"
      },
      "source": [
        "#Imputing missing data\n",
        "\n",
        "* can use mean of other known values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwMzDzqVL2ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert '?' to NaN\n",
        "df[df == '?'] = np.nan\n",
        "\n",
        "# Print the number of NaNs\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Print shape of original DataFrame\n",
        "print(\"Shape of Original DataFrame: {}\".format(df.shape))\n",
        "\n",
        "# Drop missing values and print shape of new DataFrame\n",
        "df = df.dropna()\n",
        "\n",
        "# Print shape of new DataFrame\n",
        "print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2M3OXi6L2Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Imputer module\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Setup the Imputation transformer: imp\n",
        "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
        "\n",
        "# Instantiate the SVC classifier: clf\n",
        "clf = SVC()\n",
        "\n",
        "# Setup the pipeline with the required steps: steps\n",
        "steps = [('imputation', imp),\n",
        "        ('SVM', clf)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgvZIH7nSWA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary modules\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Setup the pipeline steps: steps\n",
        "steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
        "        ('SVM', SVC())]\n",
        "\n",
        "# Create the pipeline: pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fit the pipeline to the train set\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Compute metrics\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lnlFNtIx67g",
        "colab_type": "text"
      },
      "source": [
        "#Ways to Normalize Data\n",
        "\n",
        "* **Standardization** - subtract the mean & divide by variance\n",
        ">* all have variance of 1 & centered around 0\n",
        "* subtract the minimum & divide by range\n",
        ">* makes range 0 to 1\n",
        "* Can normalize for -1 to 1 range as well\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFvmWwUOSVzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import scale\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "# Scale the features: X_scaled\n",
        "X_scaled = scale(X)\n",
        "\n",
        "# Print the mean and standard deviation of the unscaled features\n",
        "print(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \n",
        "print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n",
        "\n",
        "# Print the mean and standard deviation of the scaled features\n",
        "print(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \n",
        "print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyQcxpsg4OpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Setup the pipeline steps: steps\n",
        "steps = [('scaler', StandardScaler()),\n",
        "        ('knn', KNeighborsClassifier())]\n",
        "        \n",
        "# Create the pipeline: pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fit the pipeline to the training set: knn_scaled\n",
        "knn_scaled = pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Instantiate and fit a k-NN classifier to the unscaled data\n",
        "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
        "\n",
        "# Compute and print metrics\n",
        "print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\n",
        "print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHf2U8dc6QJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup the pipeline\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('SVM', SVC())]\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Specify the hyperparameter space\n",
        "parameters = {'SVM__C':[1, 10, 100],\n",
        "              'SVM__gamma':[0.1, 0.01]}\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
        "\n",
        "# Instantiate the GridSearchCV object: cv\n",
        "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
        "\n",
        "# Fit to the training set\n",
        "cv.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred\n",
        "y_pred = cv.predict(X_test)\n",
        "\n",
        "# Compute and print metrics\n",
        "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtdUoG_G7dRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup the pipeline steps: steps\n",
        "steps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
        "         ('scaler', StandardScaler()),\n",
        "         ('elasticnet', ElasticNet())]\n",
        "\n",
        "# Create the pipeline: pipeline \n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Specify the hyperparameter space\n",
        "parameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Create the GridSearchCV object: gm_cv\n",
        "gm_cv = GridSearchCV(pipeline, parameters)\n",
        "\n",
        "# Fit to the training set\n",
        "gm_cv.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print the metrics\n",
        "r2 = gm_cv.score(X_test, y_test)\n",
        "print(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n",
        "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfOY1bc57tJS",
        "colab_type": "text"
      },
      "source": [
        "#SUMMARY of  TECHNIQUES LEARNED\n",
        "\n",
        "* Basics of using machine Learning to build predictive models\n",
        ">* Classification and Regression with real-world data\n",
        "\n",
        "* Under-fitting and over-fiting\n",
        "* Train-Test Split\n",
        "* Cross-Validation\n",
        "* Grid Search\n",
        "* Regularization, lasso, & ridge regression"
      ]
    }
  ]
}